import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import { knowledgeService } from '@/utils/knowledgeService'

// Initialize OpenAI client
const getOpenAIClient = (apiKey: string) => {
  return new OpenAI({
    apiKey: apiKey,
  })
}

// POST endpoint for LLM Chat Completion
export async function POST(request: NextRequest) {
  try {
    const { 
      messages, 
      systemPrompt, 
      model = 'gpt-4', 
      temperature = 0.7, 
      apiKey,
      userContext 
    } = await request.json()
    
    if (!apiKey) {
      return NextResponse.json(
        { error: 'Cl√© API OpenAI manquante' },
        { status: 400 }
      )
    }
    
    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json(
        { error: 'Messages manquants ou format invalide' },
        { status: 400 }
      )
    }

    const openai = getOpenAIClient(apiKey)
    
    console.log('üß† Chat API: Generating response for', messages.length, 'messages')
    
    // R√©cup√©rer le dernier message utilisateur pour recherche contextuelle
    const lastUserMessage = messages.filter((msg: { role: string }) => msg.role === 'user').pop()
    
    // Enrichir le prompt syst√®me avec la base de connaissances
    const enhancedSystemPrompt = await knowledgeService.buildEnhancedSystemPrompt(
      systemPrompt || 'Tu es un assistant virtuel fran√ßais professionnel et serviable.'
    )
    
    // Ajouter contexte sp√©cifique si pertinent
    let contextualInfo = ''
    if (lastUserMessage) {
      contextualInfo = knowledgeService.getRelevantKnowledge(lastUserMessage.content)
    }
    
    // Prepare messages for OpenAI
    const formattedMessages = [
      {
        role: 'system' as const,
        content: enhancedSystemPrompt + contextualInfo
      },
      ...messages.map((msg: { role: 'user' | 'assistant'; content: string }) => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content
      }))
    ]
    
    // Add user context to system message if provided
    if (userContext) {
      formattedMessages[0].content += `\n\nCONTEXTE UTILISATEUR:\n${JSON.stringify(userContext, null, 2)}`
    }
    
    console.log('üìö Chat API: Enhanced with knowledge base, prompt length:', formattedMessages[0].content.length)
    
    const completion = await openai.chat.completions.create({
      model: model,
      messages: formattedMessages,
      temperature: temperature,
      max_tokens: 300, // R√©ponses plus courtes et rapides
      stream: false
    })

    const responseContent = completion.choices[0]?.message?.content || 'D√©sol√©, je n\'ai pas pu g√©n√©rer de r√©ponse.'
    
    console.log('‚úÖ Chat API: Response generated:', responseContent.substring(0, 100) + '...')
    
    return NextResponse.json({
      response: responseContent,
      model: model,
      usage: completion.usage,
      success: true
    })
    
  } catch (error: unknown) {
    console.error('‚ùå Chat API Error:', error)
    
    let errorMessage = 'Erreur lors de la g√©n√©ration de r√©ponse'
    let statusCode = 500
    
    const errorObj = error as { status?: number; message?: string }
    
    if (errorObj.status === 400) {
      errorMessage = 'Param√®tres invalides - V√©rifiez les messages et le mod√®le'
      statusCode = 400
    } else if (errorObj.status === 401) {
      errorMessage = 'Cl√© API OpenAI invalide ou expir√©e'
      statusCode = 401
    } else if (errorObj.status === 429) {
      errorMessage = 'Limite de taux OpenAI d√©pass√©e - Veuillez patienter'
      statusCode = 429
    } else if (errorObj.status === 402) {
      errorMessage = 'Quota OpenAI insuffisant - V√©rifiez votre facturation'
      statusCode = 402
    }
    
    return NextResponse.json(
      { error: errorMessage, details: errorObj.message || 'Erreur inconnue', success: false },
      { status: statusCode }
    )
  }
}